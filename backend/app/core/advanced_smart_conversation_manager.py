import asyncio
from typing import Dict, Any, List, Optional
import json
import re
import os
from pathlib import Path
from datetime import datetime
from app.core.ai_manager import AIManager

class IntentRecognizer:
    """æ„å›¾è¯†åˆ«å™¨ - åŸºäºæŸ¥è¯¢å†…å®¹æ™ºèƒ½é€‰æ‹©æ–‡ä»¶"""
    
    FILE_TYPE_PATTERNS = {
        'python': ['.py', 'requirements.txt', 'setup.py', 'pyproject.toml'],
        'javascript': ['.js', '.ts', '.jsx', '.tsx', 'package.json'],
        'config': ['.json', '.yaml', '.yml', '.toml', '.ini', '.env'],
        'documentation': ['.md', '.txt', 'README', 'LICENSE', 'CHANGELOG'],
        'build': ['Dockerfile', 'Makefile', 'docker-compose.yml', '.gitignore']
    }
    
    KEYWORD_MAPPINGS = {
        'åº“': ['requirements.txt', 'package.json', 'pyproject.toml', 'setup.py'],
        'å¯¼å…¥': ['.py', '.js', '.ts'],  # æºä»£ç æ–‡ä»¶
        'ä¾èµ–': ['requirements.txt', 'package.json', 'pyproject.toml'],
        'é…ç½®': ['.env', 'config/', 'settings/', '.json', '.yaml'],
        'æ–‡æ¡£': ['README.md', 'docs/', '.md'],
        'å‡½æ•°': ['.py', '.js', '.ts'],  # æºä»£ç æ–‡ä»¶
        'ç±»': ['.py', '.js', '.ts'],   # æºä»£ç æ–‡ä»¶
        'æ¨¡å—': ['.py', '.js', '.ts'], # æºä»£ç æ–‡ä»¶
        'åŒ…': ['requirements.txt', 'package.json'],
        'å®‰è£…': ['requirements.txt', 'package.json', 'setup.py'],
        'ä»£ç ': ['.py', '.js', '.ts', '.java', '.cpp', '.c'],
        'é¡¹ç›®': ['README.md', 'package.json', 'pyproject.toml'],
        'æ¶æ„': ['README.md', 'docs/', 'architecture.md'],
        'æµ‹è¯•': ['test/', 'tests/', '.test.', '.spec.'],
        # æ–°å¢å…³é”®è¯æ˜ å°„ï¼ˆåŸºäºClineç†å¿µï¼‰
        'readme': ['README.md', 'readme.md', 'Readme.md', 'README.txt'],
        'è¯´æ˜': ['README.md', 'è¯´æ˜.md', 'è¯´æ˜.txt', 'documentation/'],
        'ä»‹ç»': ['README.md', 'ä»‹ç».md', 'ä»‹ç».txt'],
        'æŒ‡å—': ['README.md', 'æŒ‡å—.md', 'guide.md', 'tutorial.md'],
        'å¸®åŠ©': ['README.md', 'å¸®åŠ©.md', 'help.md', 'docs/'],
        'æ•™ç¨‹': ['README.md', 'æ•™ç¨‹.md', 'tutorial.md', 'docs/'],
        'æ‰‹å†Œ': ['README.md', 'æ‰‹å†Œ.md', 'manual.md', 'docs/'],
        'è®¾ç½®': ['.env', 'config/', 'settings/', '.json', '.yaml'],
        'ç¯å¢ƒ': ['.env', 'environment.', 'env.'],
        'ä¸»æ–‡ä»¶': ['main.', 'index.', 'app.', 'src/main.'],
        'å…¥å£': ['main.', 'index.', 'app.', 'src/main.']
    }

    def __init__(self):
        # ä¸å†ä¾èµ– project_mcp_serverï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
        pass

    def extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆåŸºäºClineç†å¿µçš„æ™ºèƒ½åˆ†è¯ï¼‰"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ä½†ä¿ç•™é‡è¦ç¬¦å·å¦‚. / _ -
        cleaned_query = re.sub(r'[^\w\s./_-]', ' ', query)
        
        # åˆ†è¯å¹¶ä¿ç•™é‡è¦æŠ€æœ¯æœ¯è¯­
        words = cleaned_query.split()
        
        # æ›´æ™ºèƒ½çš„åœç”¨è¯è¿‡æ»¤
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'è¿™', 'é‚£', 
            'å“ªäº›', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'è¯·', 'åˆ†æ', 'è¿™ä¸ª', 'é¡¹ç›®', 'æ–‡ä»¶'
        }
        
        # ä¿ç•™æŠ€æœ¯ç›¸å…³è¯æ±‡å’Œæ–‡ä»¶è·¯å¾„ç›¸å…³è¯æ±‡
        keywords = [
            word for word in words 
            if (word not in stop_words and len(word) > 1) or
               any(char in word for char in ['.', '/', '_', '-'])  # ä¿ç•™æ–‡ä»¶è·¯å¾„ç›¸å…³è¯æ±‡
        ]
        
        return keywords
    
    def identify_file_types(self, keywords: List[str]) -> List[str]:
        """æ ¹æ®å…³é”®è¯è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼ˆåŸºäºClineç†å¿µçš„æ™ºèƒ½è¯†åˆ«ï¼‰"""
        file_types = set()
        
        # æ–‡æ¡£ç±»å…³é”®è¯ä¼˜å…ˆè¯†åˆ«
        doc_keywords = {'readme', 'æ–‡æ¡£', 'è¯´æ˜', 'ä»‹ç»', 'æŒ‡å—', 'å¸®åŠ©', 'æ•™ç¨‹', 'æ‰‹å†Œ', 'md', 'txt'}
        if any(keyword in doc_keywords for keyword in keywords):
            file_types.add('documentation')
        
        # é…ç½®ç±»å…³é”®è¯
        config_keywords = {'é…ç½®', 'è®¾ç½®', 'config', 'setting', 'json', 'yaml', 'yml', 'env'}
        if any(keyword in config_keywords for keyword in keywords):
            file_types.add('config')
        
        # æºä»£ç ç±»å…³é”®è¯
        code_keywords = {'ä»£ç ', 'å‡½æ•°', 'ç±»', 'æ¨¡å—', 'åŒ…', 'py', 'js', 'ts', 'java', 'æºç '}
        if any(keyword in code_keywords for keyword in keywords):
            file_types.add('python')
            file_types.add('javascript')
        
        # æ„å»ºç±»å…³é”®è¯
        build_keywords = {'æ„å»º', 'ç¼–è¯‘', 'å®‰è£…', 'éƒ¨ç½²', 'docker', 'makefile'}
        if any(keyword in build_keywords for keyword in keywords):
            file_types.add('build')
        
        # åŸºäºæ–‡ä»¶æ‰©å±•åçš„è¯†åˆ«
        for keyword in keywords:
            for file_type, patterns in self.FILE_TYPE_PATTERNS.items():
                if any(pattern in keyword for pattern in patterns):
                    file_types.add(file_type)
        
        return list(file_types) if file_types else ['python', 'javascript', 'config', 'documentation']  # é»˜è®¤åŒ…å«æ–‡æ¡£ç±»å‹

    async def get_project_structure(self, project_path: str) -> Dict[str, Any]:
        """è·å–é¡¹ç›®æ–‡ä»¶ç»“æ„"""
        try:
            files = []
            project_path_obj = Path(project_path)

            if not project_path_obj.exists():
                return {"name": "project", "type": "directory", "children": []}

            # é€’å½’æ”¶é›†æ–‡ä»¶
            def collect_files_recursively(directory: Path, max_depth: int = 10, current_depth: int = 0):
                if current_depth >= max_depth:
                    return

                try:
                    for item in directory.iterdir():
                        # è·³è¿‡éšè—ç›®å½•å’Œå¸¸è§çš„å¿½ç•¥ç›®å½•
                        if item.name.startswith('.') or item.name in ['node_modules', '__pycache__', 'venv', 'env', '.git']:
                            continue

                        if item.is_file():
                            files.append({
                                "path": str(item.relative_to(project_path_obj)),
                                "name": item.name,
                                "size": item.stat().st_size
                            })
                        elif item.is_dir():
                            collect_files_recursively(item, max_depth, current_depth + 1)
                except PermissionError:
                    pass

            collect_files_recursively(project_path_obj)

            if files:
                return self._build_file_tree(files)
            else:
                return {"name": "project", "type": "directory", "children": []}

        except Exception as e:
            print(f"è·å–é¡¹ç›®ç»“æ„å¤±è´¥: {str(e)}")
            return {"name": "project", "type": "directory", "children": []}
    
    def _build_file_tree(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°†æ–‡ä»¶åˆ—è¡¨æ„å»ºä¸ºæ ‘å½¢ç»“æ„"""
        root = {"name": "project", "type": "directory", "children": []}
        
        for item in files:
            path_parts = item["path"].split('/')
            current_level = root["children"]
            
            for i, part in enumerate(path_parts):
                if i == len(path_parts) - 1:
                    # è¿™æ˜¯æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼Œæ˜¯æ–‡ä»¶
                    current_level.append({
                        "name": part,
                        "type": "file",
                        "path": item["path"]
                    })
                else:
                    # æŸ¥æ‰¾æˆ–åˆ›å»ºç›®å½•
                    found_dir = None
                    for child in current_level:
                        if child["type"] == "directory" and child["name"] == part:
                            found_dir = child
                            break
                    
                    if not found_dir:
                        found_dir = {
                            "name": part,
                            "type": "directory",
                            "children": []
                        }
                        current_level.append(found_dir)
                    
                    current_level = found_dir["children"]
        
        return root
    
    def find_files_in_tree(self, tree: Dict[str, Any], patterns: List[str]) -> List[str]:
        """åœ¨æ–‡ä»¶æ ‘ä¸­æŸ¥æ‰¾åŒ¹é…æ¨¡å¼çš„æ–‡ä»¶"""
        files = []
        
        if tree.get("type") == "file":
            file_name = tree.get("name", "")
            if any(pattern.lower() in file_name.lower() for pattern in patterns):
                files.append(file_name)
        
        elif tree.get("type") == "directory":
            for child in tree.get("children", []):
                files.extend(self.find_files_in_tree(child, patterns))
        
        return files
    
    def _get_file_patterns_for_keyword(self, keyword: str) -> List[str]:
        """æ ¹æ®å…³é”®è¯ç”Ÿæˆæ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼ˆåŸºäºClineç†å¿µï¼‰"""
        keyword_lower = keyword.lower()
        
        # READMEç›¸å…³
        if 'readme' in keyword_lower:
            return ['README.md', 'readme.md', 'Readme.md', 'README.txt', 'readme.txt']
        
        # é…ç½®æ–‡ä»¶ç›¸å…³
        if any(term in keyword_lower for term in ['é…ç½®', 'config', 'setting']):
            return ['.env', 'config.', 'settings.', '.json', '.yaml', '.yml']
        
        # åŒ…ç®¡ç†æ–‡ä»¶
        if any(term in keyword_lower for term in ['åŒ…', 'package', 'ä¾èµ–', 'requirement']):
            return ['package.json', 'requirements.txt', 'pyproject.toml', 'setup.py']
        
        # æºä»£ç æ–‡ä»¶
        if any(term in keyword_lower for term in ['ä»£ç ', 'æºç ', 'source']):
            return ['.py', '.js', '.ts', '.java']
        
        return []

    def _handle_special_query_intents(self, query_context: str, project_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å¤„ç†ç‰¹æ®ŠæŸ¥è¯¢æ„å›¾ï¼ˆåŸºäºClineç†å¿µï¼‰"""
        suggested_files = []
        
        # READMEæŸ¥è¯¢
        if any(term in query_context for term in ['readme', 'æ–‡æ¡£', 'è¯´æ˜', 'ä»‹ç»']):
            readme_files = self.find_files_in_tree(project_structure, ['README.md', 'readme.md', 'è¯´æ˜.md', 'ä»‹ç».md'])
            for file_path in readme_files:
                suggested_files.append({
                    "file_path": file_path,
                    "reason": "æ–‡æ¡£æŸ¥è¯¢åŒ¹é…",
                    "priority": 18
                })
        
        # ä¸»å…¥å£æ–‡ä»¶æŸ¥è¯¢
        if any(term in query_context for term in ['ä¸»æ–‡ä»¶', 'å…¥å£', 'main', 'index']):
            main_files = self.find_files_in_tree(project_structure, ['main.', 'index.', 'app.', 'src/main.'])
            for file_path in main_files:
                suggested_files.append({
                    "file_path": file_path,
                    "reason": "ä¸»å…¥å£æ–‡ä»¶åŒ¹é…",
                    "priority": 16
                })
        
        return suggested_files

    def match_files_to_query(self, project_structure: Dict[str, Any], keywords: List[str], file_types: List[str]) -> List[Dict[str, Any]]:
        """åŒ¹é…æŸ¥è¯¢åˆ°å…·ä½“çš„æ–‡ä»¶ï¼ˆåŸºäºClineç†å¿µçš„æ™ºèƒ½åŒ¹é…ï¼‰"""
        suggested_files = []
        
        # 1. ç²¾ç¡®æ–‡ä»¶ååŒ¹é…ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰- æ”¹è¿›ç‰ˆ
        for keyword in keywords:
            file_patterns = self._get_file_patterns_for_keyword(keyword)
            if file_patterns:
                matched_files = self.find_files_in_tree(project_structure, file_patterns)
                for file_path in matched_files:
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": f"ç²¾ç¡®åŒ¹é… '{keyword}'",
                        "priority": 25  # æœ€é«˜ä¼˜å…ˆçº§
                    })
        
        # 2. åŸºäºæ–‡ä»¶æ‰©å±•åçš„æ™ºèƒ½åŒ¹é…
        for keyword in keywords:
            if '.' in keyword and len(keyword) > 3:  # å¯èƒ½æ˜¯æ–‡ä»¶æ‰©å±•å
                ext = keyword.lower()
                if any(ext.endswith(pat) for pat in ['.py', '.js', '.ts', '.json', '.md', '.txt', '.yaml', '.yml']):
                    matched_files = self.find_files_in_tree(project_structure, [f"*{ext}"])
                    for file_path in matched_files:
                        suggested_files.append({
                            "file_path": file_path,
                            "reason": f"æ–‡ä»¶æ‰©å±•ååŒ¹é… '{ext}'",
                            "priority": 20
                        })
        
        # 3. åŸºäºå…³é”®è¯çš„ç›´æ¥åŒ¹é…
        for keyword in keywords:
            if keyword in self.KEYWORD_MAPPINGS:
                patterns = self.KEYWORD_MAPPINGS[keyword]
                matched_files = self.find_files_in_tree(project_structure, patterns)
                for file_path in matched_files:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡
                    if not any(f["file_path"] == file_path for f in suggested_files):
                        suggested_files.append({
                            "file_path": file_path,
                            "reason": f"å…³é”®è¯ '{keyword}' åŒ¹é…",
                            "priority": 15  # é«˜ä¼˜å…ˆçº§
                        })
        
        # 4. æ™ºèƒ½æŸ¥è¯¢æ„å›¾å¤„ç†ï¼ˆæ–°å¢ï¼‰
        query_context = " ".join(keywords).lower()
        suggested_files.extend(self._handle_special_query_intents(query_context, project_structure))
        
        # 5. åŸºäºæ–‡ä»¶ç±»å‹çš„åŒ¹é…
        for file_type in file_types:
            patterns = self.FILE_TYPE_PATTERNS[file_type]
            matched_files = self.find_files_in_tree(project_structure, patterns)
            for file_path in matched_files:
                # é¿å…é‡å¤æ·»åŠ 
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": f"æ–‡ä»¶ç±»å‹ '{file_type}' åŒ¹é…",
                        "priority": 10  # ä¸­ä¼˜å…ˆçº§
                    })
        
        # 6. ç¡®ä¿åŒ…å«å¸¸è§é…ç½®æ–‡ä»¶ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
        common_configs = ['README.md', 'package.json', 'requirements.txt', 'pyproject.toml', 'setup.py']
        for config_file in common_configs:
            config_files = self.find_files_in_tree(project_structure, [config_file])
            for file_path in config_files:
                if not any(f["file_path"] == file_path for f in suggested_files):
                    suggested_files.append({
                        "file_path": file_path,
                        "reason": "å¸¸è§é¡¹ç›®é…ç½®æ–‡ä»¶",
                        "priority": 5  # ä½ä¼˜å…ˆçº§
                    })
        
        return suggested_files
    
    def find_and_optimize_duplicate_file_reads(self, file_requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å’Œä¼˜åŒ–é‡å¤æ–‡ä»¶è¯»å–ï¼ˆåŸºäºClineç†å¿µï¼‰"""
        file_read_count = {}
        optimized_requests = []
        
        for req in file_requests:
            file_path = req["file_path"]
            file_read_count[file_path] = file_read_count.get(file_path, 0) + 1
            
            # å¦‚æœæ˜¯é‡å¤è¯»å–ï¼Œé™ä½ä¼˜å…ˆçº§æˆ–æ·»åŠ æ ‡è®°
            if file_read_count[file_path] > 1:
                optimized_req = req.copy()
                optimized_req["priority"] = max(1, req.get("priority", 10) - 5)  # é™ä½ä¼˜å…ˆçº§
                optimized_req["reason"] = f"{req.get('reason', '')} (é‡å¤è¯»å–)"
                optimized_requests.append(optimized_req)
            else:
                optimized_requests.append(req)
        
        return optimized_requests

    def prioritize_and_deduplicate(self, files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜å…ˆçº§æ’åºå’Œå»é‡ï¼ˆåŸºäºClineç†å¿µï¼‰"""
        # é¦–å…ˆä¼˜åŒ–é‡å¤æ–‡ä»¶è¯»å–
        optimized_files = self.find_and_optimize_duplicate_file_reads(files)
        
        # å»é‡
        unique_files = {}
        for file_info in optimized_files:
            file_path = file_info["file_path"]
            if file_path not in unique_files or file_info["priority"] > unique_files[file_path]["priority"]:
                unique_files[file_path] = file_info
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_files = sorted(unique_files.values(), key=lambda x: x["priority"], reverse=True)
        
        return sorted_files
    
    async def analyze_query(self, project_path: str, query: str) -> List[Dict[str, Any]]:
        """åˆ†ææŸ¥è¯¢å¹¶è¿”å›éœ€è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨"""
        print(f"ğŸ” åˆ†ææŸ¥è¯¢: '{query}'")
        
        # 1. å…³é”®è¯æå–å’Œåˆ†ç±»
        keywords = self.extract_keywords(query.lower())
        print(f"  æå–å…³é”®è¯: {keywords}")
        
        file_types = self.identify_file_types(keywords)
        print(f"  è¯†åˆ«æ–‡ä»¶ç±»å‹: {file_types}")
        
        # 2. è·å–é¡¹ç›®ç»“æ„
        project_structure = await self.get_project_structure(project_path)
        
        # 3. æ™ºèƒ½æ–‡ä»¶åŒ¹é…
        suggested_files = self.match_files_to_query(project_structure, keywords, file_types)
        print(f"  åŒ¹é…åˆ° {len(suggested_files)} ä¸ªæ–‡ä»¶")
        
        # 4. ä¼˜å…ˆçº§æ’åºå’Œå»é‡
        prioritized_files = self.prioritize_and_deduplicate(suggested_files)
        print(f"  å»é‡åå‰©ä½™ {len(prioritized_files)} ä¸ªæ–‡ä»¶")
        
        return prioritized_files[:8]  # è¿”å›æœ€å¤š8ä¸ªæ–‡ä»¶


class AutoFileReader:
    """è‡ªåŠ¨æ–‡ä»¶è¯»å–å™¨ - æ— éœ€ç”¨æˆ·æ‰¹å‡†"""

    def __init__(self):
        # ä¸å†ä¾èµ– project_mcp_serverï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶ç³»ç»Ÿæ“ä½œ
        pass

    async def read_files(self, project_path: str, file_requests: List[Dict[str, Any]]) -> Dict[str, str]:
        """è‡ªåŠ¨è¯»å–å¤šä¸ªæ–‡ä»¶"""
        file_contents = {}
        
        print(f"ğŸ“– å¼€å§‹è¯»å– {len(file_requests)} ä¸ªæ–‡ä»¶...")
        
        for file_request in file_requests:
            file_path = file_request["file_path"]
            reason = file_request.get("reason", "")
            
            try:
                content = await self.read_file(project_path, file_path)
                if content:
                    file_contents[file_path] = content
                    print(f"   âœ“ {file_path} - {reason}")
                else:
                    print(f"   âš ï¸ {file_path} - æ–‡ä»¶ä¸ºç©º")
            except Exception as e:
                print(f"   âŒ {file_path} - è¯»å–å¤±è´¥: {str(e)}")
                # å°è¯•è·¯å¾„ä¿®æ­£
                corrected_path = await self.try_correct_path(project_path, file_path)
                if corrected_path:
                    try:
                        content = await self.read_file(project_path, corrected_path)
                        if content:
                            file_contents[corrected_path] = content
                            print(f"   âœ“ {corrected_path} - è·¯å¾„ä¿®æ­£åæˆåŠŸè¯»å–")
                    except:
                        print(f"   âŒ {corrected_path} - è·¯å¾„ä¿®æ­£åä»ç„¶å¤±è´¥")
        
        print(f"âœ… æˆåŠŸè¯»å– {len(file_contents)}/{len(file_requests)} ä¸ªæ–‡ä»¶")
        return file_contents
    
    async def read_file(self, project_path: str, file_path: str) -> Optional[str]:
        """è¯»å–å•ä¸ªæ–‡ä»¶"""
        try:
            full_path = Path(project_path) / file_path

            if not full_path.exists() or not full_path.is_file():
                return None

            # å°è¯•ä¸åŒç¼–ç è¯»å–æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'latin-1']
            for encoding in encodings:
                try:
                    content = full_path.read_text(encoding=encoding)
                    return content
                except UnicodeDecodeError:
                    continue

            return None
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¼‚å¸¸ {file_path}: {str(e)}")
            return None
    
    async def try_correct_path(self, project_path: str, original_path: str) -> Optional[str]:
        """å°è¯•ä¿®æ­£æ–‡ä»¶è·¯å¾„"""
        # ç®€å•çš„è·¯å¾„ä¿®æ­£é€»è¾‘
        corrections = [
            original_path,
            original_path.lower(),
            original_path.upper(),
            "./" + original_path,
            original_path.lstrip('./'),
        ]

        # å¯¹äºå¸¸è§çš„é…ç½®æ–‡ä»¶ï¼Œå°è¯•æ ‡å‡†ä½ç½®
        common_files = {
            'requirements.txt': ['requirements.txt', 'reqs.txt'],
            'package.json': ['package.json', 'package-lock.json'],
            'README.md': ['README.md', 'readme.md', 'Readme.md'],
            'pyproject.toml': ['pyproject.toml'],
            'setup.py': ['setup.py']
        }

        if original_path in common_files:
            corrections.extend(common_files[original_path])

        # æµ‹è¯•æ¯ä¸ªä¿®æ­£åçš„è·¯å¾„
        for corrected_path in corrections:
            if corrected_path == original_path:
                continue

            try:
                full_path = Path(project_path) / corrected_path
                if full_path.exists() and full_path.is_file():
                    return corrected_path
            except:
                continue

        return None


class FileContextTracker:
    """æ–‡ä»¶ä¸Šä¸‹æ–‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.read_history = {}  # æ–‡ä»¶è¯»å–å†å²
        self.project_context = {}  # é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    def track_file_read(self, file_path: str, content: str):
        """è®°å½•æ–‡ä»¶è¯»å–å†å²"""
        preview = content[:200] + "..." if len(content) > 200 else content
        self.read_history[file_path] = {
            "timestamp": datetime.now().isoformat(),
            "preview": preview,
            "content_length": len(content)
        }
    
    def get_relevant_context(self, current_query: str) -> Dict[str, Any]:
        """è·å–ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        relevant_files = {}
        keywords = current_query.lower().split()
        
        for file_path, info in self.read_history.items():
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            file_match = any(keyword in file_path.lower() for keyword in keywords)
            content_match = any(keyword in info["preview"].lower() for keyword in keywords)
            
            if file_match or content_match:
                relevant_files[file_path] = info
        
        return relevant_files
    
    def get_read_history(self) -> Dict[str, Any]:
        """è·å–å®Œæ•´çš„è¯»å–å†å²"""
        return self.read_history


class AdvancedSmartConversationManager:
    """é«˜çº§æ™ºèƒ½å¯¹è¯ç®¡ç†å™¨ - ç±»Clineæ¶æ„"""
    
    def __init__(self):
        self.ai_manager = AIManager()
        self.intent_recognizer = IntentRecognizer()
        self.file_reader = AutoFileReader()
        self.file_context_tracker = FileContextTracker()
        self.conversations = {}
    
    def _get_ai_config(self) -> Dict[str, Any]:
        """è·å–AIé…ç½® - æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°è¯»å–é…ç½®æ–‡ä»¶"""
        try:
            import json
            import os
            
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = os.path.dirname(__file__)
            
            # æ„å»ºæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä»å½“å‰æ–‡ä»¶ä½ç½®è®¡ç®—ï¼‰
            config_paths = [
                os.path.join(current_dir, '..', 'api', 'AI-Config.json'),  # backend/app/api/AI-Config.json
                os.path.join(current_dir, '..', '..', 'api', 'AI-Config.json'),  # backend/api/AI-Config.json
                os.path.join(current_dir, '..', '..', '..', 'AI-Config.json'),   # AI-Config.json
            ]
            
            # æ·»åŠ ç»å¯¹è·¯å¾„æ£€æŸ¥å¹¶å»é‡
            abs_config_paths = []
            for path in config_paths:
                abs_path = os.path.abspath(path)
                if abs_path not in abs_config_paths:
                    abs_config_paths.append(abs_path)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ” æŸ¥æ‰¾AIé…ç½®æ–‡ä»¶ï¼Œæ£€æŸ¥ä»¥ä¸‹è·¯å¾„:")
            for i, path in enumerate(abs_config_paths, 1):
                exists = os.path.exists(path)
                print(f"  {i}. {path} - {'âœ… å­˜åœ¨' if exists else 'âŒ ä¸å­˜åœ¨'}")
            
            config = None
            config_path = None
            for path in abs_config_paths:
                if os.path.exists(path):
                    config_path = path
                    print(f"ğŸ“„ è¯»å–AIé…ç½®æ–‡ä»¶: {path}")
                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            config = json.load(f)
                        print(f"âœ… æˆåŠŸè¯»å–é…ç½®æ–‡ä»¶")
                        break
                    except Exception as e:
                        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ {path}: {str(e)}")
                        continue
            
            if config:
                print(f"ğŸ¤– ä½¿ç”¨AIé…ç½®: {config.get('ai_provider', 'unknown')}")
                # éªŒè¯å¿…è¦çš„é…ç½®å­—æ®µ
                api_key = config.get("ai_api_key", "")
                if not api_key:
                    print("âš ï¸ è­¦å‘Š: AIé…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘api_key")
                
                return {
                    "provider": config.get("ai_provider", "openai"),
                    "model": config.get("ai_model", "gpt-4o-mini"),
                    "api_key": api_key,
                    "base_url": config.get("ai_base_url")
                }
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            print("â„¹ï¸ æœªæ‰¾åˆ°AIé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡")
            env_api_key = os.getenv("AI_API_KEY", "")
            if not env_api_key:
                print("âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ä¸­ç¼ºå°‘AI_API_KEY")
            
            return {
                "provider": os.getenv("AI_PROVIDER", "openai"),
                "model": os.getenv("AI_MODEL", "gpt-4o-mini"),
                "api_key": env_api_key,
                "base_url": os.getenv("AI_BASE_URL")
            }
            
        except Exception as e:
            print(f"âŒ è¯»å–AIé…ç½®å¤±è´¥: {str(e)}")
            return {
                "provider": "openai",
                "model": "gpt-4o-mini",
                "api_key": "",
                "base_url": None
            }
    
    async def generate_response(self, project_path: str, user_query: str, file_contents: Dict[str, str], context: Dict[str, Any] = None) -> str:
        """åŸºäºæ–‡ä»¶å†…å®¹ç”Ÿæˆå›ç­”"""
        # æ„å»ºæ–‡ä»¶å†…å®¹æ‘˜è¦
        file_summary = "\n".join([
            f"æ–‡ä»¶: {file_path}\nå†…å®¹:\n{content[:1000]}...\n{'-'*50}"
            for file_path, content in file_contents.items()
        ])
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if context:
            context_info = f"\nç›¸å…³ä¸Šä¸‹æ–‡æ–‡ä»¶:\n" + "\n".join([
                f"- {file_path} (ä¸Šæ¬¡è¯»å–: {info['timestamp']})"
                for file_path, info in context.items()
            ]) + "\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡ä»¶å†…å®¹ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {user_query}
é¡¹ç›®è·¯å¾„: {project_path}
{context_info}
ç›¸å…³æ–‡ä»¶å†…å®¹:
{file_summary}

è¯·ç›´æ¥é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®çš„ç­”æ¡ˆï¼Œä¿æŒå›ç­”ç®€æ´ã€ç›´æ¥ã€å®ç”¨ã€‚
è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è‡ªç„¶æ˜“æ‡‚ã€‚"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºä»£ç æ–‡ä»¶å†…å®¹æä¾›æ·±å…¥çš„é¡¹ç›®åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        ai_config = self._get_ai_config()
        
        response = await self.ai_manager.chat(
            provider=ai_config["provider"],
            model=ai_config["model"],
            messages=messages,
            api_key=ai_config["api_key"],
            base_url=ai_config.get("base_url"),
            temperature=0.7,
            max_tokens=1500
        )
        
        return response["content"]
    
    async def process_smart_chat(self, conversation_id: str, project_path: str, user_query: str) -> Dict[str, Any]:
        """å¤„ç†æ™ºèƒ½å¯¹è¯è¯·æ±‚"""
        print(f"\nğŸ¤– === é«˜çº§æ™ºèƒ½åˆ†æå¼€å§‹ ===")
        print(f"ä¼šè¯ID: {conversation_id}")
        print(f"é¡¹ç›®è·¯å¾„: {project_path}")
        print(f"ç”¨æˆ·æŸ¥è¯¢: '{user_query}'")
        print("=" * 50)
        
        try:
            # 1. æ„å›¾åˆ†æå’Œæ–‡ä»¶é€‰æ‹©
            print("ğŸ” é˜¶æ®µ1: åˆ†ææŸ¥è¯¢æ„å›¾å’Œé€‰æ‹©æ–‡ä»¶...")
            file_requests = await self.intent_recognizer.analyze_query(project_path, user_query)
            
            if not file_requests:
                print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶åˆ—è¡¨")
                file_requests = [
                    {"file_path": "README.md", "reason": "é»˜è®¤é¡¹ç›®æ–‡æ¡£", "priority": 1},
                    {"file_path": "package.json", "reason": "é»˜è®¤é¡¹ç›®é…ç½®", "priority": 1}
                ]
            
            print(f"ğŸ“ é€‰æ‹©çš„æ–‡ä»¶:")
            for i, req in enumerate(file_requests, 1):
                print(f"  {i}. {req['file_path']} - {req.get('reason', 'æ— åŸå› ')}")
            
            # 2. è‡ªåŠ¨æ–‡ä»¶è¯»å–
            print("\nğŸ“– é˜¶æ®µ2: è‡ªåŠ¨è¯»å–æ–‡ä»¶...")
            file_contents = await self.file_reader.read_files(project_path, file_requests)
            
            # 3. æ›´æ–°ä¸Šä¸‹æ–‡
            for file_path, content in file_contents.items():
                self.file_context_tracker.track_file_read(file_path, content)
            
            # 4. è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context = self.file_context_tracker.get_relevant_context(user_query)
            
            # 5. æ™ºèƒ½å›ç­”ç”Ÿæˆ
            print("\nğŸ’¡ é˜¶æ®µ3: ç”Ÿæˆæ™ºèƒ½å›ç­”...")
            response_content = await self.generate_response(project_path, user_query, file_contents, context)
            
            # 6. è¿”å›ç»“æœ
            print("âœ… åˆ†æå®Œæˆ")
            
            # æ„å»ºå·¥å…·è°ƒç”¨ç»“æœ
            tool_calls = []
            for req in file_requests:
                file_path = req["file_path"]
                tool_calls.append({
                    "tool_name": "read_project_file",
                    "arguments": {"project_path": project_path, "file_path": file_path},
                    "result": {
                        "success": file_path in file_contents,
                        "content": file_contents.get(file_path, ""),
                        "file_path": file_path  # ç¡®ä¿åŒ…å«æ–‡ä»¶è·¯å¾„
                    },
                    "reason": req.get("reason", ""),
                    "file_path": file_path  # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°é¡¶å±‚ï¼Œä¾¿äºå‰ç«¯æ˜¾ç¤º
                })
            
            return {
                "response": response_content,
                "tool_calls": tool_calls,
                "conversation_id": conversation_id,
                "analysis_context": {
                    "query": user_query,
                    "selected_files": file_requests,
                    "successful_reads": len(file_contents),
                    "context_files": list(context.keys())
                }
            }
            
        except Exception as e:
            error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"âŒ é”™è¯¯: {error_msg}")
            print("=" * 50)
            return {
                "response": error_msg,
                "tool_calls": [],
                "conversation_id": conversation_id,
                "error": str(e)
            }


# åˆ›å»ºå…¨å±€å®ä¾‹
advanced_smart_conversation_manager = AdvancedSmartConversationManager()
