"""
ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥

å‚è€ƒ Cline çš„å®ç°ï¼š
1. æ¸è¿›å¼å‹ç¼©ï¼šå…ˆä¼˜åŒ–æ–‡ä»¶è¯»å–ï¼Œå†æˆªæ–­æ¶ˆæ¯
2. æ™ºèƒ½æ–‡ä»¶å¤„ç†ï¼šè¯†åˆ«å¹¶æ ‡è®°é‡å¤çš„æ–‡ä»¶è¯»å–ï¼ˆè€Œéåˆ é™¤ï¼‰
3. åˆ†å±‚æ¶ˆæ¯ç®¡ç†ï¼šä¿ç•™å…³é”®æ¶ˆæ¯ï¼Œå‹ç¼©å†å²æ¶ˆæ¯
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from enum import Enum

from app.core.context.token_counter import TokenCounter
from app.core.ai_manager import AIManager
from app.core.context.file_read_tracker import (
    FileReadTracker,
    extract_file_reads_from_messages,
    replace_duplicate_file_reads
)

logger = logging.getLogger(__name__)


class CompressionLevel(Enum):
    """å‹ç¼©çº§åˆ«"""
    NONE = "none"  # ä¸å‹ç¼©
    LIGHT = "light"  # è½»åº¦å‹ç¼©ï¼ˆä¿ç•™ 75%ï¼‰
    MEDIUM = "medium"  # ä¸­åº¦å‹ç¼©ï¼ˆä¿ç•™ 50%ï¼‰
    AGGRESSIVE = "aggressive"  # æ¿€è¿›å‹ç¼©ï¼ˆä¿ç•™ 25%ï¼‰


class CompressionStrategy:
    """
    ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥

    å‚è€ƒ Cline çš„ ContextManager å®ç°
    """

    # å‹ç¼©é˜ˆå€¼ï¼ˆå‚è€ƒ Clineï¼Œæ›´æ¿€è¿›åœ°åœ¨æ—©æœŸå‹ç¼©ï¼‰
    SHOULD_COMPRESS_THRESHOLD = 0.5  # å½“ä½¿ç”¨é‡è¶…è¿‡ 50% æ—¶å‹ç¼©ï¼ˆåŸæ¥æ˜¯ 80%ï¼‰
    MUST_COMPRESS_THRESHOLD = 0.7  # å½“ä½¿ç”¨é‡è¶…è¿‡ 70% æ—¶å¼ºåˆ¶å‹ç¼©ï¼ˆåŸæ¥æ˜¯ 95%ï¼‰

    # ä¿ç•™ç­–ç•¥
    KEEP_FIRST_N_PAIRS = 1  # ä¿ç•™å‰ N è½®å¯¹è¯
    KEEP_LAST_N_PAIRS = 2  # ä¿ç•™æœ€å N è½®å¯¹è¯

    def __init__(self, ai_manager: Optional[AIManager] = None):
        self.ai_manager = ai_manager or AIManager()
        self.token_counter = TokenCounter()
        self.file_read_tracker = FileReadTracker()

    def should_compress(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        threshold: Optional[float] = None
    ) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©

        Args:
            messages: å½“å‰æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            threshold: è‡ªå®šä¹‰é˜ˆå€¼

        Returns:
            æ˜¯å¦éœ€è¦å‹ç¼©
        """
        threshold = threshold or self.SHOULD_COMPRESS_THRESHOLD
        info = self.token_counter.get_compression_info(messages, model)
        return info["usage_percentage"] >= threshold

    def must_compress(self, messages: List[Dict[str, Any]], model: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¿…é¡»å‹ç¼©"""
        info = self.token_counter.get_compression_info(messages, model)
        return info["must_compress"]

    async def compress_conversation_history(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        ai_config: Dict[str, Any],
        compression_level: Optional[CompressionLevel] = None
    ) -> List[Dict[str, Any]]:
        """
        å‹ç¼©å¯¹è¯å†å²

        å‚è€ƒ Cline çš„ä¸¤é˜¶æ®µç­–ç•¥ï¼š
        1. **ç¬¬ä¸€é˜¶æ®µï¼šä¼˜åŒ–é‡å¤æ–‡ä»¶è¯»å–**ï¼ˆä¸åˆ é™¤æ¶ˆæ¯ï¼Œåªæ›¿æ¢å†…å®¹ï¼‰
           - æ£€æµ‹é‡å¤çš„æ–‡ä»¶è¯»å–
           - å°†æ—§çš„è¯»å–æ›¿æ¢ä¸ºç®€çŸ­æç¤º
           - ä¿ç•™æœ€æ–°çš„æ–‡ä»¶è¯»å–å†…å®¹
           - è¿™æ · AI ä»ç„¶çŸ¥é“å·²ç»è¯»è¿‡è¿™äº›æ–‡ä»¶

        2. **ç¬¬äºŒé˜¶æ®µï¼šä¸‰æ˜æ²»æˆªæ–­**ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
           - ä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼ˆå§‹ç»ˆä¿ç•™ï¼‰
           - ä¿ç•™ç¬¬ä¸€è½®ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤ï¼ˆä»»åŠ¡èµ·ç‚¹ï¼‰
           - åˆ é™¤ä¸­é—´çš„å†å²æ¶ˆæ¯
           - ä¿ç•™æœ€åå‡ è½®å¯¹è¯ï¼ˆæœ€è¿‘çŠ¶æ€ï¼‰

        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            ai_config: AI é…ç½®
            compression_level: å‹ç¼©çº§åˆ«

        Returns:
            å‹ç¼©åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if len(messages) <= 4:
            # æ¶ˆæ¯å¤ªå°‘ï¼Œä¸éœ€è¦å‹ç¼©
            return messages

        print(f"\nğŸ—œï¸  å¼€å§‹å‹ç¼©ä¸Šä¸‹æ–‡...")
        print(f"   - åŸå§‹æ¶ˆæ¯æ•°: {len(messages)}")

        # === é˜¶æ®µ 1: ä¼˜åŒ–é‡å¤æ–‡ä»¶è¯»å–ï¼ˆCline çš„å…³é”®ç­–ç•¥ï¼‰===
        print(f"\nğŸ“– é˜¶æ®µ 1: æ‰«ææ–‡ä»¶è¯»å–...")

        # æå–æ‰€æœ‰æ–‡ä»¶è¯»å–
        file_reads = extract_file_reads_from_messages(messages)
        print(f"   - å‘ç° {len(file_reads)} æ¬¡æ–‡ä»¶è¯»å–")

        # è®°å½•åˆ°è¿½è¸ªå™¨
        for file_path, msg_idx, content_idx, content_length in file_reads:
            self.file_read_tracker.record_file_read(file_path, msg_idx, content_idx, content_length)

        # æ‰“å°åˆ†ææŠ¥å‘Š
        report = self.file_read_tracker.get_optimization_report()
        print(report)

        # å¦‚æœæœ‰é‡å¤è¯»å–ï¼Œè¿›è¡Œä¼˜åŒ–
        optimized_messages = messages
        if self.file_read_tracker.should_optimize(threshold_savings=1000):
            print(f"\nâœ¨ é˜¶æ®µ 1b: ä¼˜åŒ–é‡å¤æ–‡ä»¶è¯»å–...")
            optimized_messages = replace_duplicate_file_reads(messages, self.file_read_tracker)

            # è®¡ç®—èŠ‚çœ
            savings = self.file_read_tracker.calculate_savings()
            print(f"   âœ… å·²æ›¿æ¢ {savings['file_count']} ä¸ªæ–‡ä»¶çš„é‡å¤è¯»å–")
            print(f"   âœ… èŠ‚çœçº¦ {savings['total_savings']:,} å­—ç¬¦")

        # æ£€æŸ¥ä¼˜åŒ–åæ˜¯å¦è¿˜éœ€è¦æˆªæ–­
        total_chars = sum(len(str(msg.get("content", ""))) for msg in optimized_messages)
        MAX_TOTAL_CHARS = 40_000  # GLM æ¨¡å‹é™åˆ¶

        if total_chars <= MAX_TOTAL_CHARS:
            print(f"\nâœ… ä¼˜åŒ–åå­—ç¬¦æ•°: {total_chars:,} (åœ¨é™åˆ¶å†…)")
            return optimized_messages

        # === é˜¶æ®µ 2: ä¸‰æ˜æ²»æˆªæ–­ï¼ˆä»…åœ¨ä¼˜åŒ–åä»ç„¶è¶…é™æ—¶ï¼‰===
        print(f"\nğŸ“Š é˜¶æ®µ 2: æ¶ˆæ¯æˆªæ–­...")
        print(f"   - å½“å‰å­—ç¬¦æ•°: {total_chars:,} > {MAX_TOTAL_CHARS:,}")
        print(f"   - éœ€è¦æˆªæ–­å†å²æ¶ˆæ¯...")

        # 1. æå–ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        system_messages = [msg for msg in optimized_messages if msg.get("role") == "system"]
        non_system_messages = [msg for msg in optimized_messages if msg.get("role") != "system"]

        if len(non_system_messages) <= 4:
            # éç³»ç»Ÿæ¶ˆæ¯å¤ªå°‘ï¼Œä¸éœ€è¦å‹ç¼©
            return optimized_messages

        # 2. Cline çš„"ä¸‰æ˜æ²»æˆªæ–­æ³•"
        compressed = []

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        compressed.extend(system_messages)

        # ä¿ç•™ç¬¬ä¸€è½®å¯¹è¯ï¼ˆå‰ 2 æ¡æ¶ˆæ¯ï¼šuser + assistantï¼‰
        first_pair = non_system_messages[:2]
        compressed.extend(first_pair)

        # ç¡®å®šè¦ä¿ç•™çš„æœ€åå‡ è½®å¯¹è¯
        # å‚è€ƒ Cline: keep="lastTwo" è¡¨ç¤ºä¿ç•™æœ€å 1 è½®å¯¹è¯ï¼ˆ2 æ¡æ¶ˆæ¯ï¼‰
        if compression_level == CompressionLevel.AGGRESSIVE:
            # æ¿€è¿›å‹ç¼©ï¼šåªä¿ç•™ç¬¬ä¸€è½®å’Œæœ€åä¸€è½®
            last_pair_count = 2
        elif compression_level == CompressionLevel.MEDIUM:
            # ä¸­åº¦å‹ç¼©ï¼šä¿ç•™ç¬¬ä¸€è½®å’Œæœ€åä¸¤è½®
            last_pair_count = 4
        else:  # LIGHT
            # è½»åº¦å‹ç¼©ï¼šä¿ç•™ç¬¬ä¸€è½®å’Œæœ€å 4 è½®
            last_pair_count = 8

        # æ·»åŠ æœ€åçš„å¯¹è¯
        if len(non_system_messages) > 2 + last_pair_count:
            last_pairs = non_system_messages[-last_pair_count:]
            compressed.extend(last_pairs)
        else:
            # å¦‚æœæ¶ˆæ¯æ€»æ•°ä¸å¤Ÿï¼Œå°±å…¨éƒ¨ä¿ç•™
            compressed.extend(non_system_messages[2:])

        # è®¡ç®—æœ€ç»ˆå­—ç¬¦æ•°
        final_chars = sum(len(str(msg.get("content", ""))) for msg in compressed)
        print(f"   - æˆªæ–­åæ¶ˆæ¯æ•°: {len(compressed)}")
        print(f"   - æˆªæ–­åå­—ç¬¦æ•°: {final_chars:,}")
        print(f"   - åˆ é™¤äº† {len(optimized_messages) - len(compressed)} æ¡å†å²æ¶ˆæ¯")
        print(f"   - æ€»å‹ç¼©ç‡: {(1 - len(compressed)/len(messages)) * 100:.1f}%")

        return compressed

    def _determine_compression_level(
        self,
        messages: List[Dict[str, Any]],
        model: str
    ) -> CompressionLevel:
        """æ ¹æ® token ä½¿ç”¨é‡ç¡®å®šå‹ç¼©çº§åˆ«"""
        info = self.token_counter.get_compression_info(messages, model)
        usage = info["usage_percentage"]

        if usage >= self.MUST_COMPRESS_THRESHOLD:
            return CompressionLevel.AGGRESSIVE
        elif usage >= self.SHOULD_COMPRESS_THRESHOLD:
            return CompressionLevel.MEDIUM
        else:
            return CompressionLevel.LIGHT

    async def _summarize_messages(
        self,
        messages: List[Dict[str, Any]],
        ai_config: Dict[str, Any]
    ) -> Optional[str]:
        """
        ä½¿ç”¨ AI æ€»ç»“æ¶ˆæ¯

        å‚è€ƒ Cline çš„ SummarizeTask å·¥å…·å®ç°
        """
        try:
            # æ„å»ºæ€»ç»“æç¤ºè¯
            summarize_prompt = self._build_summarize_prompt()

            # å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ–‡æœ¬
            messages_text = self._format_messages_for_summary(messages)

            # è°ƒç”¨ AI
            response = await self.ai_manager.chat(
                provider=ai_config.get("ai_provider", "deepseek"),
                model=ai_config.get("ai_model", "deepseek-chat"),
                messages=[{
                    "role": "user",
                    "content": f"{summarize_prompt}\n\nå¯¹è¯å†å²ï¼š\n{messages_text}"
                }],
                api_key=ai_config.get("ai_api_key"),
                base_url=ai_config.get("ai_base_url"),
                temperature=0.3,
                max_tokens=2000
            )

            if response and response.get("content"):
                return response["content"].strip()

        except Exception as e:
            logger.error(f"AI æ€»ç»“å¤±è´¥: {e}")

        return None

    def _build_summarize_prompt(self) -> str:
        """æ„å»ºæ€»ç»“æç¤ºè¯ï¼ˆå‚è€ƒ Cline çš„ summarizeTaskï¼‰"""
        return """è¯·è¯¦ç»†æ€»ç»“ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

1. **ç”¨æˆ·çš„ä¸»è¦è¯·æ±‚**: ç”¨æˆ·æƒ³è¦å®Œæˆä»€ä¹ˆä»»åŠ¡ï¼Ÿ

2. **æŠ€æœ¯æ¦‚å¿µ**: è®¨è®ºäº†å“ªäº›æŠ€æœ¯æ¦‚å¿µã€æ¡†æ¶æˆ–å·¥å…·ï¼Ÿ

3. **å…³é”®æ–‡ä»¶**: æŸ¥çœ‹æˆ–ä¿®æ”¹äº†å“ªäº›æ–‡ä»¶ï¼Ÿï¼ˆåˆ—å‡ºæ–‡ä»¶è·¯å¾„ï¼‰

4. **é—®é¢˜è§£å†³**: è§£å†³äº†å“ªäº›é—®é¢˜ï¼Ÿå¦‚ä½•è§£å†³çš„ï¼Ÿ

5. **å½“å‰çŠ¶æ€**: ç›®å‰ä»»åŠ¡è¿›è¡Œåˆ°å“ªä¸€æ­¥äº†ï¼Ÿ

6. **å¾…åŠäº‹é¡¹**: è¿˜æœ‰å“ªäº›æœªå®Œæˆçš„ä»»åŠ¡ï¼Ÿ

è¯·ç®€æ´ä½†å®Œæ•´åœ°æ€»ç»“ï¼Œä¾¿äºç»§ç»­å®Œæˆä»»åŠ¡ã€‚"""

    def _format_messages_for_summary(self, messages: List[Dict[str, Any]]) -> str:
        """å°†æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºé€‚åˆæ€»ç»“çš„æ–‡æœ¬"""
        parts = []
        for msg in messages:
            role = msg.get("role", "unknown")
            content = msg.get("content", "")

            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 500:
                content = content[:500] + "...(å†…å®¹å·²æˆªæ–­)"

            parts.append(f"{role.upper()}: {content}")

        return "\n\n".join(parts)

    def truncate_tool_result(self, tool_name: str, result: Any, max_chars: int = 5000) -> str:
        """
        æˆªæ–­è¿‡é•¿çš„å·¥å…·ç»“æœ

        å‚è€ƒ Cline çš„ç»ˆç«¯è¾“å‡ºæˆªæ–­é€»è¾‘
        """
        result_str = str(result)

        if len(result_str) <= max_chars:
            return result_str

        # ä¿ç•™å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†
        half = max_chars // 2
        truncated = f"{result_str[:half]}\n\n... (ç»“æœå·²æˆªæ–­ï¼ŒåŸé•¿åº¦: {len(result_str)} å­—ç¬¦) ...\n\n{result_str[-half:]}"

        return truncated

    def optimize_file_reads(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä¼˜åŒ–æ–‡ä»¶è¯»å–æ¶ˆæ¯

        å‚è€ƒ Cline çš„æ–‡ä»¶è¯»å–ä¼˜åŒ–ï¼š
        1. è¯†åˆ«é‡å¤çš„æ–‡ä»¶è¯»å–
        2. å°†é‡å¤çš„è¯»å–æ›¿æ¢ä¸ºå¼•ç”¨
        """
        # TODO: å®ç°æ–‡ä»¶è¯»å–ä¼˜åŒ–
        # è¿™éœ€è¦è§£æå·¥å…·è°ƒç”¨ç»“æœï¼Œè¯†åˆ« file_content å—
        return messages

    def get_compression_stats(
        self,
        original: List[Dict[str, Any]],
        compressed: List[Dict[str, Any]],
        model: str
    ) -> Dict[str, Any]:
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        original_tokens = self.token_counter.count_messages_tokens(original)
        compressed_tokens = self.token_counter.count_messages_tokens(compressed)

        return {
            "original_messages": len(original),
            "compressed_messages": len(compressed),
            "original_tokens": original_tokens,
            "compressed_tokens": compressed_tokens,
            "tokens_saved": original_tokens - compressed_tokens,
            "compression_ratio": (1 - compressed_tokens / original_tokens) if original_tokens > 0 else 0,
            "context_window": self.token_counter.get_context_window(model),
            "max_allowed": self.token_counter.get_max_allowed_size(model),
        }
