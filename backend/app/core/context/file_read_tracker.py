"""
æ–‡ä»¶è¯»å–å†å²è¿½è¸ªå™¨

å‚è€ƒ Cline çš„ ContextManager å®ç°ï¼š
- æ£€æµ‹é‡å¤çš„æ–‡ä»¶è¯»å–
- å°†é‡å¤è¯»å–æ›¿æ¢ä¸ºç®€çŸ­æç¤º
- ä¿ç•™æœ€æ–°çš„æ–‡ä»¶è¯»å–å†…å®¹
"""

import re
import logging
from typing import Dict, List, Any, Set, Tuple
from collections import defaultdict

logger = logging.getLogger(__name__)


class FileReadTracker:
    """
    æ–‡ä»¶è¯»å–è¿½è¸ªå™¨

    è¿½è¸ªæ‰€æœ‰æ–‡ä»¶è¯»å–æ“ä½œï¼Œè¯†åˆ«é‡å¤è¯»å–ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®
    """

    # é‡å¤æ–‡ä»¶è¯»å–çš„æç¤ºæ–‡æœ¬ï¼ˆå‚è€ƒ Clineï¼‰
    DUPLICATE_FILE_READ_NOTICE = (
        "[NOTE] æ­¤æ–‡ä»¶è¯»å–å·²è¢«ç§»é™¤ä»¥èŠ‚çœä¸Šä¸‹æ–‡çª—å£ç©ºé—´ã€‚"
        "è¯·å‚è€ƒæœ€æ–°çš„æ–‡ä»¶è¯»å–ä»¥è·å–æ­¤æ–‡ä»¶çš„æœ€æ–°ç‰ˆæœ¬ã€‚"
    )

    def __init__(self):
        # {file_path: [(message_index, content_index, original_length)]}
        self.file_read_history: Dict[str, List[Tuple[int, int, int]]] = defaultdict(list)

    def record_file_read(
        self,
        file_path: str,
        message_index: int,
        content_index: int,
        content_length: int
    ):
        """
        è®°å½•æ–‡ä»¶è¯»å–

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            message_index: åœ¨æ¶ˆæ¯åˆ—è¡¨ä¸­çš„ç´¢å¼•
            content_index: åœ¨æ¶ˆæ¯å†…å®¹ä¸­çš„ç´¢å¼•
            content_length: å†…å®¹é•¿åº¦
        """
        self.file_read_history[file_path].append((message_index, content_index, content_length))
        logger.debug(f"è®°å½•æ–‡ä»¶è¯»å–: {file_path} (ç´¢å¼•: {message_index}, é•¿åº¦: {content_length})")

    def get_duplicate_file_reads(self) -> Dict[str, List[Tuple[int, int, int]]]:
        """
        è·å–æ‰€æœ‰é‡å¤çš„æ–‡ä»¶è¯»å–ï¼ˆå‡ºç°æ¬¡æ•° > 1ï¼‰

        Returns:
            {file_path: [(message_index, content_index, length), ...]}
        """
        duplicates = {
            path: indices
            for path, indices in self.file_read_history.items()
            if len(indices) > 1
        }
        return duplicates

    def calculate_savings(self) -> Dict[str, Any]:
        """
        è®¡ç®—å¦‚æœæ›¿æ¢é‡å¤è¯»å–èƒ½èŠ‚çœçš„å­—ç¬¦æ•°

        Returns:
            {
                "total_savings": æ€»èŠ‚çœå­—ç¬¦æ•°,
                "file_count": æ¶‰åŠçš„æ–‡ä»¶æ•°é‡,
                "read_count": æ¶‰åŠçš„è¯»å–æ¬¡æ•°,
                "files": {file_path: savings}
            }
        """
        duplicates = self.get_duplicate_file_reads()
        total_savings = 0
        total_reads = 0
        file_details = {}

        for file_path, indices in duplicates.items():
            # ä¿ç•™æœ€åä¸€æ¬¡è¯»å–ï¼Œæ›¿æ¢ä¹‹å‰çš„æ‰€æœ‰è¯»å–
            # åªæ›¿æ¢ indices[:-1]ï¼Œä¿ç•™ indices[-1]
            file_savings = sum(length for _, _, length in indices[:-1])

            # æ›¿æ¢ä¸ºæç¤ºæ–‡æœ¬çš„é•¿åº¦
            notice_length = len(self.DUPLICATE_FILE_READ_NOTICE)
            replacement_cost = notice_length * (len(indices) - 1)

            actual_savings = max(0, file_savings - replacement_cost)

            total_savings += actual_savings
            total_reads += len(indices)

            file_details[file_path] = {
                "read_count": len(indices),
                "savings": actual_savings,
                "original_size": sum(length for _, _, length in indices),
                "indices": [idx for idx, _, _ in indices]
            }

        return {
            "total_savings": total_savings,
            "file_count": len(duplicates),
            "read_count": total_reads,
            "files": file_details
        }

    def should_optimize(self, threshold_savings: int = 5000) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œä¼˜åŒ–

        Args:
            threshold_savings: èŠ‚çœå­—ç¬¦æ•°é˜ˆå€¼

        Returns:
            æ˜¯å¦åº”è¯¥ä¼˜åŒ–
        """
        savings = self.calculate_savings()
        return savings["total_savings"] >= threshold_savings

    def reset(self):
        """é‡ç½®è¿½è¸ªå™¨"""
        self.file_read_history.clear()

    def get_optimization_report(self) -> str:
        """è·å–ä¼˜åŒ–æŠ¥å‘Šï¼ˆç”¨äºæ—¥å¿—ï¼‰"""
        savings = self.calculate_savings()

        if savings["file_count"] == 0:
            return "æ²¡æœ‰æ£€æµ‹åˆ°é‡å¤çš„æ–‡ä»¶è¯»å–"

        lines = [
            f"\nğŸ“Š æ–‡ä»¶è¯»å–ä¼˜åŒ–åˆ†æ:",
            f"   - é‡å¤è¯»å–çš„æ–‡ä»¶: {savings['file_count']} ä¸ª",
            f"   - æ€»è¯»å–æ¬¡æ•°: {savings['read_count']} æ¬¡",
            f"   - å¯èŠ‚çœå­—ç¬¦æ•°: {savings['total_savings']:,} å­—ç¬¦",
            f"\n   è¯¦ç»†ä¿¡æ¯:"
        ]

        for file_path, details in savings["files"].items():
            lines.append(
                f"   - {file_path}:"
                f" è¯»å– {details['read_count']} æ¬¡, "
                f"å¯èŠ‚çœ {details['savings']:,} å­—ç¬¦"
            )

        return "\n".join(lines)


def extract_file_reads_from_messages(
    messages: List[Dict[str, Any]]
) -> List[Tuple[str, int, int, int]]:
    """
    ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–æ‰€æœ‰æ–‡ä»¶è¯»å–

    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨

    Returns:
        [(file_path, message_index, content_index, content_length), ...]
    """
    file_reads = []

    # æ–‡ä»¶è¯»å–çš„æ¨¡å¼ï¼ˆä»å·¥å…·ç»“æœä¸­æå–ï¼‰
    # æ ¼å¼1: [read_file for 'path/to/file'] Result: content
    pattern1 = r"\[read_file\s+for\s+'([^']+)'\]\s+Result:"

    # æ ¼å¼2: <file_content path="path/to/file">content</file_content>
    pattern2 = r'<file_content\s+path="([^"]+)">'

    for msg_idx, message in enumerate(messages):
        if message.get("role") != "user":
            continue

        content = message.get("content", "")
        if not isinstance(content, str):
            continue

        # å°è¯•åŒ¹é…æ ¼å¼1
        match1 = re.match(pattern1, content)
        if match1:
            file_path = match1.group(1)
            content_length = len(content)
            file_reads.append((file_path, msg_idx, 0, content_length))
            continue

        # å°è¯•åŒ¹é…æ ¼å¼2
        match2 = re.search(pattern2, content)
        if match2:
            file_path = match2.group(1)
            content_length = len(content)
            file_reads.append((file_path, msg_idx, 0, content_length))

    return file_reads


def replace_duplicate_file_reads(
    messages: List[Dict[str, Any]],
    tracker: FileReadTracker
) -> List[Dict[str, Any]]:
    """
    æ›¿æ¢é‡å¤çš„æ–‡ä»¶è¯»å–ä¸ºç®€çŸ­æç¤º

    Args:
        messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
        tracker: æ–‡ä»¶è¯»å–è¿½è¸ªå™¨

    Returns:
        ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    duplicates = tracker.get_duplicate_file_reads()

    if not duplicates:
        return messages

    # åˆ›å»ºæ¶ˆæ¯å‰¯æœ¬ï¼ˆæ·±æ‹·è´ï¼‰
    import copy
    optimized_messages = copy.deepcopy(messages)

    for file_path, indices in duplicates.items():
        # ä¿ç•™æœ€åä¸€æ¬¡è¯»å–ï¼Œæ›¿æ¢ä¹‹å‰çš„æ‰€æœ‰è¯»å–
        for msg_idx, content_idx, _ in indices[:-1]:
            if msg_idx < len(optimized_messages):
                message = optimized_messages[msg_idx]

                # æ›¿æ¢ä¸ºæç¤ºæ–‡æœ¬
                if message.get("role") == "user":
                    content = message.get("content", "")

                    # æ ¼å¼1: [read_file for 'path'] Result: content
                    pattern1 = rf"\[read_file\s+for\s+'{re.escape(file_path)}'\]\s+Result:.*"
                    replacement1 = f"[read_file for '{file_path}'] Result:\n{tracker.DUPLICATE_FILE_READ_NOTICE}"

                    # æ ¼å¼2: <file_content path="path">content</file_content>
                    pattern2 = rf'<file_content\s+path="{re.escape(file_path)}">[\s\S]*?</file_content>'
                    replacement2 = f'<file_content path="{file_path}">{tracker.DUPLICATE_FILE_READ_NOTICE}</file_content>'

                    # å°è¯•æ›¿æ¢
                    new_content = re.sub(pattern1, replacement1, content, flags=re.DOTALL)
                    if new_content == content:
                        new_content = re.sub(pattern2, replacement2, content, flags=re.DOTALL)

                    if new_content != content:
                        message["content"] = new_content
                        logger.debug(f"æ›¿æ¢é‡å¤æ–‡ä»¶è¯»å–: {file_path} (æ¶ˆæ¯ç´¢å¼•: {msg_idx})")

    return optimized_messages
